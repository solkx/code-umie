output_dir: ./output/firefly-llama2-13b-base
model_name_or_path: NousResearch/Llama-2-13b-hf
tokenizer_name_or_path: ziqingyang/chinese-llama-2-13b
data_path: ./data
train_embedding: true # word embedding与lm_head是否参与训练

num_train_epochs: 1
max_steps: -1
per_device_train_batch_size: 6
gradient_accumulation_steps: 2

min_seq_length: 300
max_seq_length: 1024
window_step_size: 1024

learning_rate: 0.0002
logging_steps: 200
save_steps: 500
save_total_limit: 3
lr_scheduler_type: cosine
warmup_steps: 1000
# load_best_model_at_end: true  # 最终保存loss最小的checkpoint

lora_rank: 64
lora_alpha: 16
lora_dropout: 0.05

gradient_checkpointing: true
disable_tqdm: false
optim: paged_adamw_32bit
seed: 42
fp16: true
report_to: tensorboard
dataloader_num_workers: 0
save_strategy: steps
weight_decay: 0
max_grad_norm: 0.3
remove_unused_columns: false
# label_names: []